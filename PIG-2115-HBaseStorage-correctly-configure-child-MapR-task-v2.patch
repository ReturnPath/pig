diff --git a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
index d65abbd..4e772f0 100644
--- a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
+++ b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
@@ -76,6 +76,7 @@ import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.StoreFuncInterface;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat.HBaseTableIFBuilder;
+import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.builtin.Utf8StorageConverter;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -508,14 +509,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
     @Override
     public void setLocation(String location, Job job) throws IOException {
         job.getConfiguration().setBoolean("pig.noSplitCombination", true);
-        m_conf = job.getConfiguration();
-        HBaseConfiguration.addHbaseResources(m_conf);
-
-        // Make sure the HBase, ZooKeeper, and Guava jars get shipped.
-        TableMapReduceUtil.addDependencyJars(job.getConfiguration(), 
-            org.apache.hadoop.hbase.client.HTable.class,
-            com.google.common.collect.Lists.class,
-            org.apache.zookeeper.ZooKeeper.class);
+        m_conf = initialiseHBaseClassLoaderResources(job);
 
         String tablename = location;
         if (location.startsWith("hbase://")){
@@ -551,6 +545,19 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         m_conf.set(TableInputFormat.SCAN, convertScanToString(scan));
     }
 
+    private Configuration initialiseHBaseClassLoaderResources(Job job) {
+        Configuration hbaseConfig = HBaseConfiguration.create();
+        ConfigurationUtil.mergeConf(hbaseConfig, job.getConfiguration());
+
+        // Make sure the HBase, ZooKeeper, and Guava jars get shipped.
+        TableMapReduceUtil.addDependencyJars(job.getConfiguration(),
+            org.apache.hadoop.hbase.client.HTable.class,
+            com.google.common.collect.Lists.class,
+            org.apache.zookeeper.ZooKeeper.class);
+
+        return hbaseConfig;
+    }
+
     @Override
     public String relativeToAbsolutePath(String location, Path curDir)
     throws IOException {
@@ -731,8 +738,8 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         Properties props = UDFContext.getUDFContext().getUDFProperties(getClass(), new String[]{contextSignature});
         if (!props.containsKey(contextSignature + "_schema")) {
             props.setProperty(contextSignature + "_schema",  ObjectSerializer.serialize(schema_));
-    }
-        m_conf = HBaseConfiguration.addHbaseResources(job.getConfiguration());
+        }
+        m_conf = initialiseHBaseClassLoaderResources(job);
     }
 
     @Override
