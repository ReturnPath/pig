diff --git a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
index c7577ea..6b52ca2 100644
--- a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
+++ b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
@@ -21,12 +21,7 @@ import java.io.DataOutputStream;
 import java.io.IOException;
 import java.io.DataInput;
 import java.io.DataOutput;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableMap;
-import java.util.HashMap;
-import java.util.Properties;
+import java.util.*;
 
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.cli.CommandLineParser;
@@ -49,6 +44,7 @@ import org.apache.hadoop.hbase.filter.FilterList;
 import org.apache.hadoop.hbase.filter.RowFilter;
 import org.apache.hadoop.hbase.filter.FamilyFilter;
 import org.apache.hadoop.hbase.filter.ColumnPrefixFilter;
+import org.apache.hadoop.hbase.filter.PrefixFilter;
 import org.apache.hadoop.hbase.filter.Filter;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
@@ -88,6 +84,8 @@ import org.apache.pig.impl.util.UDFContext;
 
 import com.google.common.collect.Lists;
 
+import static org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.mergeConf;
+
 /**
  * A HBase implementation of LoadFunc and StoreFunc.
  * <P>
@@ -128,7 +126,8 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
     private final static String CASTER_PROPERTY = "pig.hbase.caster";
     private final static String ASTERISK = "*";
     private final static String COLON = ":";
-    
+    private static final String ROWKEY_PREFIXES_DELIMITER = ",";
+
     private List<ColumnInfo> columnInfo_ = Lists.newArrayList();
     private HTable m_table;
     private Configuration m_conf;
@@ -142,9 +141,16 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
     private final static Options validOptions_ = new Options();
     private final static CommandLineParser parser_ = new GnuParser();
     private boolean loadRowKey_;
+    private boolean loadAsTimeStampVersionTuples_;
     private final long limit_;
     private final int caching_;
     private final boolean noWAL_;
+    private boolean omitNulls_;
+    private final long scanStartTimeStamp_;
+    private final long scanEndTimeStamp_;
+    private final int scanMaxVersions_;
+    private final long putTimeStamp_;
+    private final String rowKeyFilterPrefixes_;
 
     protected transient byte[] gt_;
     protected transient byte[] gte_;
@@ -156,12 +162,15 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
     private ResourceSchema schema_;
     private RequiredFieldList requiredFieldList;
     private boolean initialized = false;
+    private Iterator currentRowVersions;
+    private Result currentResult;
 
     private static void populateValidOptions() { 
         validOptions_.addOption("loadKey", false, "Load Key");
+        validOptions_.addOption("loadAsTimeStampVersionTuples", false, "Load tuples flattened on Row timestamp versions");
         validOptions_.addOption("gt", true, "Records must be greater than this value " +
                 "(binary, double-slash-escaped)");
-        validOptions_.addOption("lt", true, "Records must be less than this value (binary, double-slash-escaped)");   
+        validOptions_.addOption("lt", true, "Records must be less than this value (binary, double-slash-escaped)");
         validOptions_.addOption("gte", true, "Records must be greater than or equal to this value");
         validOptions_.addOption("lte", true, "Records must be less than or equal to this value");
         validOptions_.addOption("caching", true, "Number of rows scanners should cache");
@@ -169,6 +178,14 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         validOptions_.addOption("caster", true, "Caster to use for converting values. A class name, " +
                 "HBaseBinaryConverter, or Utf8StorageConverter. For storage, casters must implement LoadStoreCaster.");
         validOptions_.addOption("noWAL", false, "Sets the write ahead to false for faster loading. To be used with extreme caution since this could result in data loss (see http://hbase.apache.org/book.html#perf.hbase.client.putwal).");
+        validOptions_.addOption("omitNulls", false, "Used to omit non-existent columns during load rather than " +
+                "presenting null values. For storage, avoid storing nulls in hbase, instead omit the column");
+        validOptions_.addOption("scanMaxVersions", true, "Maximum versions to be returned by the current scan");
+        validOptions_.addOption("scanStartTimestamp", true, "Record version timestamps must be greater than this value");
+        validOptions_.addOption("scanEndTimestamp", true, "Record version timestamps must be lesser than this value");
+        validOptions_.addOption("rowKeyPrefixes", true, "Records rowkeys must start with this prefix");
+        validOptions_.addOption("putTimestamp", true, "Specify the Put timestamp to be used for the current records stored in HBase. " +
+                "If not specified, it will default to current timestamp.");
     }
 
     /**
@@ -219,7 +236,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
             configuredOptions_ = parser_.parse(validOptions_, optsArr);
         } catch (ParseException e) {
             HelpFormatter formatter = new HelpFormatter();
-            formatter.printHelp( "[-loadKey] [-gt] [-gte] [-lt] [-lte] [-columnPrefix] [-caching] [-caster] [-noWAL] [-limit]", validOptions_ );
+            formatter.printHelp( "[-loadKey] [-loadAsTimeStampVersionTuples] [-gt] [-gte] [-lt] [-lte] [-columnPrefix] [-caching] [-caster] [-noWAL] [-limit] [-omitNulls] [-scanMaxVersions] [-scanStartTimestamp] [-scanEndTimestamp] [-rowKeyPrefixes] [-putTimestamp]", validOptions_ );
             throw e;
         }
 
@@ -227,6 +244,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         for (String colName : colNames) {
             columnInfo_.add(new ColumnInfo(colName));
         }
+        loadAsTimeStampVersionTuples_  = configuredOptions_.hasOption("loadAsTimeStampVersionTuples");
 
         m_conf = HBaseConfiguration.create();
         String defaultCaster = m_conf.get(CASTER_PROPERTY, STRING_CASTER);
@@ -250,10 +268,16 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         caching_ = Integer.valueOf(configuredOptions_.getOptionValue("caching", "100"));
         limit_ = Long.valueOf(configuredOptions_.getOptionValue("limit", "-1"));
         noWAL_ = configuredOptions_.hasOption("noWAL");
+        rowKeyFilterPrefixes_ = configuredOptions_.getOptionValue("rowKeyPrefixes");
+        scanStartTimeStamp_ = Long.valueOf(configuredOptions_.getOptionValue("scanStartTimestamp", "-1"));
+        scanEndTimeStamp_ = Long.valueOf(configuredOptions_.getOptionValue("scanEndTimestamp", "-1"));
+        scanMaxVersions_ = Integer.valueOf(configuredOptions_.getOptionValue("scanMaxVersions", "1"));
+        omitNulls_ = configuredOptions_.hasOption("omitNulls");
+        putTimeStamp_ = Long.valueOf(configuredOptions_.getOptionValue("putTimestamp", Long.toString(System.currentTimeMillis())));
         initScan();	    
     }
 
-    private void initScan() {
+    private void initScan() throws IOException {
         scan = new Scan();
         // Set filters, if any.
         if (configuredOptions_.hasOption("gt")) {
@@ -272,6 +296,24 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
             lte_ = Bytes.toBytesBinary(Utils.slashisize(configuredOptions_.getOptionValue("lte")));
             addRowFilter(CompareOp.LESS_OR_EQUAL, lte_);
         }
+        if (configuredOptions_.hasOption("rowKeyPrefixes")) {
+            FilterList filters = new FilterList(FilterList.Operator.MUST_PASS_ONE);
+            for( String rowKeyFilterPrefix : rowKeyFilterPrefixes_.split(ROWKEY_PREFIXES_DELIMITER) ) {
+                 filters.addFilter(new PrefixFilter(Bytes.toBytes(rowKeyFilterPrefix)));
+            }
+            scan.setFilter(filters);
+        }
+        if (configuredOptions_.hasOption("scanStartTimestamp") ) {
+            if (configuredOptions_.hasOption("scanEndTimestamp")) {
+                scan.setTimeRange(scanStartTimeStamp_, scanEndTimeStamp_);
+            } else {
+                scan.setTimeStamp(scanStartTimeStamp_);
+            }
+            scan.setMaxVersions();
+        }
+        if (configuredOptions_.hasOption("scanMaxVersions")) {
+            scan.setMaxVersions(scanMaxVersions_);
+        }
 
         // apply any column filters
         FilterList allColumnFilters = null;
@@ -305,6 +347,15 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         }
     }
 
+    private boolean isNullBytes(byte[] cell) {
+        for (byte b : cell) {
+            if (b != 0x00) {
+                return false;
+            }
+        }
+        return true;
+    }
+
     private void addRowFilter(CompareOp op, byte[] val) {
         if (LOG.isInfoEnabled()) {
             LOG.info("Adding filter " + op.toString() +
@@ -325,6 +376,9 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
     @Override
     public Tuple getNext() throws IOException {
         try {
+            if (loadAsTimeStampVersionTuples_) {
+               return getNextRowVersion();
+            }
             if (!initialized) {
                 Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass(),
                         new String[] {contextSignature});
@@ -393,6 +447,9 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
                         // It's a column so set the value
                         byte[] cell=result.getValue(columnInfo.getColumnFamily(),
                                                     columnInfo.getColumnName());
+                        if (omitNulls_ && isNullBytes(cell)) {
+                            continue;
+                        }
                         DataByteArray value =
                                 cell == null ? null : new DataByteArray(cell);
                         tuple.set(currentIndex, value);
@@ -413,6 +470,83 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         return null;
     }
 
+    private Tuple getNextRowVersion() throws IOException {
+        try {
+            // if there are no row versions of the previous scanned result, get the next row !
+            while ((currentRowVersions == null || !currentRowVersions.hasNext())) {
+                if (!reader.nextKeyValue()) {
+                    return  null;
+                }
+                currentResult = (Result) reader.getCurrentValue();
+                flattenVersions(currentResult);
+            }
+
+            // Get a timestamp version row view
+            Map<String, byte[]> rowValues = (Map<String, byte[]>) currentRowVersions.next();
+            int tupleSize = columnInfo_.size();
+            if (loadRowKey_) {
+                tupleSize++;
+            }
+
+            Tuple tuple=TupleFactory.getInstance().newTuple(tupleSize);
+
+            int startIndex=0;
+            if (loadRowKey_) {
+                tuple.set(0, new DataByteArray(currentResult.getRow()));
+                startIndex++;
+            }
+            for (int i = 0;i < columnInfo_.size(); ++i){
+                int currentIndex = startIndex + i;
+                ColumnInfo columnInfo = columnInfo_.get(i);
+                byte[] cell = rowValues.get(getColumn(columnInfo));
+                if (omitNulls_ && isNullBytes(cell)) {
+                    continue;
+                }
+                DataByteArray value =
+                        cell == null ? null : new DataByteArray(cell);
+                tuple.set(currentIndex, value);
+            }
+            return tuple;
+        } catch (InterruptedException e) {
+            throw new IOException(e);
+        }
+    }
+
+    private String getColumn(ColumnInfo columnInfo) {
+        return new StringBuilder().append(Bytes.toString(columnInfo.getColumnFamily())).append(COLON).append(Bytes.toString(columnInfo.getColumnName())).toString();
+    }
+
+    // Construct an iterator for row versions based on timestamp from the three level map(Map<family,Map<qualifier,Map<timestamp,value>>>) return by the Mapper
+    private void flattenVersions(Result result) throws IOException {
+        Map<Long, Map<String, byte[]>>  bagOfTimeStampVersions = new TreeMap<Long, Map<String, byte[]>>();
+
+        //  Iterate over the the Column Families
+        for(Map.Entry entry: result.getMap().entrySet()) {
+            String columnFamily = Bytes.toString((byte[])entry.getKey());
+            NavigableMap<byte[],NavigableMap<Long,byte[]>> columnValueMap = (NavigableMap<byte[],NavigableMap<Long,byte[]>>)entry.getValue();
+            // Group the column value pair based on the timestamp
+            for(Map.Entry valueEntry: columnValueMap.entrySet()) {
+                String column  = Bytes.toString((byte[])valueEntry.getKey());
+                NavigableMap<Long,byte[]> columnVersions = (NavigableMap<Long,byte[]>)valueEntry.getValue();
+                for(Map.Entry<Long,byte[]> version: columnVersions.entrySet()) {
+                    long timeStamp = version.getKey();
+                    byte[] columnValue = version.getValue();
+                    if(bagOfTimeStampVersions.containsKey(timeStamp)) {
+                        bagOfTimeStampVersions.get(timeStamp).put(columnFamily+":"+column, columnValue);
+                        continue;
+                    }
+                    Map<String, byte[]> columnValues = new TreeMap<String, byte[]>();
+                    StringBuilder keybuilder = new StringBuilder().append(columnFamily).append(COLON).append(column);
+                    columnValues.put(keybuilder.toString(), columnValue);
+                    bagOfTimeStampVersions.put(timeStamp, columnValues);
+                }
+            }
+        }
+
+        // return the iterator for ease of use
+        currentRowVersions = bagOfTimeStampVersions.values().iterator();
+    }
+
     @Override
     public InputFormat getInputFormat() {      
         TableInputFormat inputFormat = new HBaseTableIFBuilder()
@@ -439,14 +573,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
     @Override
     public void setLocation(String location, Job job) throws IOException {
         job.getConfiguration().setBoolean("pig.noSplitCombination", true);
-        m_conf = job.getConfiguration();
-        HBaseConfiguration.addHbaseResources(m_conf);
-
-        // Make sure the HBase, ZooKeeper, and Guava jars get shipped.
-        TableMapReduceUtil.addDependencyJars(job.getConfiguration(), 
-            org.apache.hadoop.hbase.client.HTable.class,
-            com.google.common.collect.Lists.class,
-            org.apache.zookeeper.ZooKeeper.class);
+        m_conf = initialiseHBaseClassLoaderResources(job);
 
         String tablename = location;
         if (location.startsWith("hbase://")){
@@ -482,6 +609,19 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         m_conf.set(TableInputFormat.SCAN, convertScanToString(scan));
     }
 
+    private Configuration initialiseHBaseClassLoaderResources(Job job) throws IOException {
+        Configuration hbaseConfig = HBaseConfiguration.create();
+        mergeConf(hbaseConfig, job.getConfiguration());
+
+        // Make sure the HBase, ZooKeeper, and Guava jars get shipped.
+        TableMapReduceUtil.addDependencyJars(job.getConfiguration(),
+            HTable.class,
+            Lists.class,
+            org.apache.zookeeper.ZooKeeper.class);
+
+        return hbaseConfig;
+    }
+
     @Override
     public String relativeToAbsolutePath(String location, Path curDir)
     throws IOException {
@@ -554,7 +694,6 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         }
         ResourceFieldSchema[] fieldSchemas = (schema_ == null) ? null : schema_.getFields();
         byte type = (fieldSchemas == null) ? DataType.findType(t.get(0)) : fieldSchemas[0].getType();
-        long ts=System.currentTimeMillis();
 
         Put put = createPut(t.get(0), type);
 
@@ -567,17 +706,22 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
 
         for (int i=1;i<t.size();++i){
             ColumnInfo columnInfo = columnInfo_.get(i-1);
+            Object storeValue = t.get(i);
+
             if (LOG.isDebugEnabled()) {
-                LOG.debug("putNext - tuple: " + i + ", value=" + t.get(i) +
+                LOG.debug("putNext - tuple: " + i + ", value=" + storeValue +
                         ", cf:column=" + columnInfo);
         }
+            if (omitNulls_ && storeValue == null) {
+                continue;
+            }
 
             if (!columnInfo.isColumnMap()) {
                 put.add(columnInfo.getColumnFamily(), columnInfo.getColumnName(),
-                        ts, objToBytes(t.get(i), (fieldSchemas == null) ?
-                        DataType.findType(t.get(i)) : fieldSchemas[i].getType()));
+                        putTimeStamp_, objToBytes(storeValue, (fieldSchemas == null) ?
+                        DataType.findType(storeValue) : fieldSchemas[i].getType()));
             } else {
-                Map<String, Object> cfMap = (Map<String, Object>) t.get(i);
+                Map<String, Object> cfMap = (Map<String, Object>) storeValue;
                 for (String colName : cfMap.keySet()) {
                     if (LOG.isDebugEnabled()) {
                         LOG.debug("putNext - colName=" + colName +
@@ -585,7 +729,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
                     }
                     // TODO deal with the fact that maps can have types now. Currently we detect types at
                     // runtime in the case of storing to a cf, which is suboptimal.
-                    put.add(columnInfo.getColumnFamily(), Bytes.toBytes(colName.toString()), ts,
+                    put.add(columnInfo.getColumnFamily(), Bytes.toBytes(colName.toString()), putTimeStamp_,
                             objToBytes(cfMap.get(colName), DataType.findType(cfMap.get(colName))));
                 }
             }
@@ -608,7 +752,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
      * @throws IOException
      */
     public Put createPut(Object key, byte type) throws IOException {
-        Put put = new Put(objToBytes(key, type));
+        Put put = new Put(objToBytes(key, type), putTimeStamp_);
 
         if(noWAL_) {
             put.setWriteToWAL(false);
@@ -663,7 +807,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         if (!props.containsKey(contextSignature + "_schema")) {
             props.setProperty(contextSignature + "_schema",  ObjectSerializer.serialize(schema_));
     }
-        m_conf = HBaseConfiguration.addHbaseResources(job.getConfiguration());
+        m_conf = initialiseHBaseClassLoaderResources(job);
     }
 
     @Override
diff --git a/test/org/apache/pig/test/data/test-timestamp-versions-scan.pig b/test/org/apache/pig/test/data/test-timestamp-versions-scan.pig
new file mode 100644
index 0000000..07ad976
--- /dev/null
+++ b/test/org/apache/pig/test/data/test-timestamp-versions-scan.pig
@@ -0,0 +1,13 @@
+ts_version_rows = LOAD 'test_table'
+	USING
+	    org.apache.pig.backend.hadoop.hbase.HBaseStorage('$column_1 $column_2 $column_3',
+	    '-caster org.apache.pig.backend.hadoop.hbase.HBaseBinaryConverter -loadKey -loadAsTimeStampVersionTuples -scanStartTimestamp $start_timestamp -scanEndTimestamp $end_timestamp -rowKeyPrefixes $rf_codes')
+    AS
+        (
+            rowkey:chararray,
+            column_1:long,
+            column_2:double,
+            column_3:double
+        );
+
+STORE ts_version_rows INTO '$hdfs_path' USING PigStorage('\t');
\ No newline at end of file
diff --git a/test/org/apache/pig/test/utils/TestHBaseStorage.java b/test/org/apache/pig/test/utils/TestHBaseStorage.java
new file mode 100644
index 0000000..174d428
--- /dev/null
+++ b/test/org/apache/pig/test/utils/TestHBaseStorage.java
@@ -0,0 +1,148 @@
+package org.apache.pig.test.utils;
+
+import static org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.toProperties;
+import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.TableExistsException;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.impl.PigContext;
+import org.junit.*;
+import org.springframework.core.io.ClassPathResource;
+import org.springframework.core.io.Resource;
+import java.io.File;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import static com.google.common.io.Files.createTempDir;
+import static org.apache.hadoop.hbase.util.Bytes.toBytes;
+import static org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.toProperties;
+import static org.junit.Assert.assertEquals;
+
+/**
+ * @author Hariprasad Kuppuswamy
+ */
+public class TestHBaseStorage {
+
+    private static final HBaseTestingUtility hbaseTestingUtil = new HBaseTestingUtility();
+    private static final String TEST_TABLE_NAME =  "test_table";
+    private static final String TEST_COLUMN_FAMILY_1 =  "test_column_family_1";
+    private static final String TEST_COLUMN_FAMILY_2 =  "test_column_family_2";
+    private static final String TEST_COLUMN_NAME_1 =  "test_column_1";
+    private static final String TEST_COLUMN_NAME_2 =  "test_column_2";
+    private static final String TEST_COLUMN_NAME_3 =  "test_column_3";
+
+    private static final byte[] TEST_TABLE_NAME_IN_BYTES = toBytes(TEST_TABLE_NAME);
+    private static final byte[] TEST_COLUMN_FAMILY_1_IN_BYTES = toBytes(TEST_COLUMN_FAMILY_1);
+    private static final byte[] TEST_COLUMN_FAMILY_2_IN_BYTES = toBytes(TEST_COLUMN_FAMILY_2);
+    private static final byte[] TEST_COLUMN_NAME_1_IN_BYTES = toBytes(TEST_COLUMN_NAME_1);
+    private static final byte[] TEST_COLUMN_NAME_2_IN_BYTES = toBytes(TEST_COLUMN_NAME_2);
+    private static final byte[] TEST_COLUMN_NAME_3_IN_BYTES = toBytes(TEST_COLUMN_NAME_3);
+
+    private static long TEST_START_TIMESTAMP = 1301612400000l;
+    private static long TEST_END_TIMESTAMP = 1401612400000l;
+
+    private final Resource pigScript = new ClassPathResource("pig/test-timestamp-versions-scan.pig");
+
+    private PigServer pigServer;
+    private Map params;
+    private String tempFile = createTempDir()+"/test.txt";
+
+    @BeforeClass
+    public static void initEmbeddedHBase() throws Exception {
+        // disable UI or it clashes with running region servers,
+        // we do not need the web UI for testing anyhow
+        hbaseTestingUtil.getConfiguration().set("hbase.regionserver.info.port", "-1");
+        hbaseTestingUtil.getConfiguration().set("hbase.master.info.port", "-1");
+        hbaseTestingUtil.startMiniCluster(1);
+    }
+
+    @Before
+    public void createTestHBaseTables() throws Exception {
+        HTable table;
+
+        try {
+            table = hbaseTestingUtil.createTable(TEST_TABLE_NAME_IN_BYTES, new byte[][] {
+                TEST_COLUMN_FAMILY_1_IN_BYTES, TEST_COLUMN_FAMILY_2_IN_BYTES }, 365 );
+        } catch (TableExistsException tee) {
+            table = hbaseTestingUtil.truncateTable(TEST_TABLE_NAME_IN_BYTES);
+        }
+
+        hbaseTestingUtil.waitTableAvailable(TEST_TABLE_NAME_IN_BYTES, 2000);
+
+        // Create dummy test data
+        table.put(generatePutCommand("rowkey", TEST_START_TIMESTAMP, 300l, null, null));
+        table.put(generatePutCommand("rowkey", TEST_START_TIMESTAMP + 1, 500l, new Double(500),  new Double(400)));
+        table.put(generatePutCommand("rowkey", TEST_START_TIMESTAMP + 2, null, new Double(100), null));
+        table.put(generatePutCommand("rowkey", TEST_START_TIMESTAMP + 3, 300l, new Double(1000),  new Double(800)));
+
+        table.put(generatePutCommand("rowkey 1", TEST_START_TIMESTAMP - 1, 0l, new Double(500), new Double(400)));
+        table.put(generatePutCommand("rowkey 2", TEST_END_TIMESTAMP + 1, 100l, new Double(100), new Double(100)));
+
+        table.put(generatePutCommand("not matching rowkey", TEST_START_TIMESTAMP, 300l, null, null));
+        table.put(generatePutCommand("not matching rowkey", TEST_START_TIMESTAMP + 1, 500l, new Double(500),  new Double(400)));
+        table.put(generatePutCommand("not matching rowkey", TEST_START_TIMESTAMP + 2, null, new Double(100), null));
+        table.put(generatePutCommand("not matching rowkey", TEST_START_TIMESTAMP + 3, 300l, new Double(1000),  new Double(800)));
+    }
+
+    private Put generatePutCommand(String keyword, long timeStamp, Long column_1_Value, Double column_2_Value, Double column_3_Value) {
+        Put put= new Put(toBytes(keyword), timeStamp);
+        if (column_1_Value != null) {
+            put.add(TEST_COLUMN_FAMILY_1_IN_BYTES, TEST_COLUMN_NAME_1_IN_BYTES, timeStamp, toBytes(column_1_Value));
+        }
+        if (column_2_Value != null) {
+            put.add(TEST_COLUMN_FAMILY_2_IN_BYTES, TEST_COLUMN_NAME_2_IN_BYTES, timeStamp, toBytes(column_2_Value));
+        }
+        if (column_3_Value != null) {
+            put.add(TEST_COLUMN_FAMILY_2_IN_BYTES, TEST_COLUMN_NAME_3_IN_BYTES, timeStamp, toBytes(column_3_Value));
+        }
+        return put;
+    }
+
+    @Before
+    public void configureTestingPigServer() throws Exception {
+        params = new HashMap();
+        params.put("table", "test_table");
+        params.put("column_1", TEST_COLUMN_FAMILY_1+":"+TEST_COLUMN_NAME_1);
+        params.put("column_2", TEST_COLUMN_FAMILY_2+":"+TEST_COLUMN_NAME_2);
+        params.put("column_3", TEST_COLUMN_FAMILY_2+":"+TEST_COLUMN_NAME_3);
+        params.put("start_timestamp", Long.toString(TEST_START_TIMESTAMP));
+        params.put("end_timestamp", Long.toString(TEST_END_TIMESTAMP));
+        params.put("rf_codes", "rowkey");
+        params.put("hdfs_path", tempFile);
+
+        // Running it in Local mode for test
+        Properties props = toProperties(hbaseTestingUtil.getConfiguration());
+        PigContext pigContext = new PigContext(ExecType.LOCAL, props);
+        pigServer = new PigServer(pigContext);
+    }
+
+    @Test
+    public void testTimeStampVersionsScan() throws Exception {
+        pigServer.registerScript(pigScript.getInputStream(), params);
+        File outputFile = new File(tempFile+"/part-m-00000");
+        List<String> lines = FileUtils.readLines(outputFile);
+        assertEquals(4, lines.size());
+        String line = lines.get(1);
+        String[] columns = line.split("\t");
+        assertEquals(4 ,columns.length);
+        assertEquals("rowkey", columns[0]);
+        assertEquals("500", columns[1]);
+        assertEquals("500.0", columns[2]);
+        assertEquals("400.0", columns[3]);
+    }
+
+    @After
+    public void tearDown() throws Exception {
+        new File(tempFile).deleteOnExit();
+    }
+
+    @AfterClass
+    public static void destroyHBaseCluster() throws Exception {
+        hbaseTestingUtil.shutdownMiniCluster();
+    }
+
+}
